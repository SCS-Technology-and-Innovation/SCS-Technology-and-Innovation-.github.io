<!DOCTYPE html>
<HTML lang="en">
  <HEAD>
    <META HTTP-EQUIV="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="../../all.min.css">
    <link rel="stylesheet" href="../../styles.min.css">
    <link rel="stylesheet" href="../../custom.css">
    <link rel="stylesheet" href="../../style-2022.css">
    <title>Introduction / Module 12 / CCCS 660</TITLE>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-F8SJ1WFY9N"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){
	  dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "G-F8SJ1WFY9N");
    </script>
  </head>
  <body class="content" role="document">
    <section class="bg-">
      <div class="container-fluid">      
	<h1>Introduction</h1>
	<p>
	  When an AI model produces an output, it is reasonable to
	  want to know <em>why</em> that output was given. Tracing
	  back the origin of the output in terms of the insides of the
	  model and the data present in the input is the process of
	  explaining the workings of the model. This is the focus of
	  explainable AI (XAI). It is much easier to do when the model
	  has been built with explainability in mind, although
	  traditional black-box models are not completely hopeless
	  either.
	</p>
	<h2 class="topic-heading">Objectives</h2>
	<p>
	  In this module, you will have an opportunity to...
	  <ul class="action-items">
	    <li>... quantify the impact of input features in the output of an AI model</li>
	    <li>... visualize the relative effects of input features in the output of an AI model</li>
	    <li>... assess the limitations of existing XAI approaches</li>
	    <li>... </li>
	  </ul>
	</p>
	<h2 class="topic-heading">Warm-up</h2>
	<p>
	  E-books on explainable AI (pick at least one and browse it for a bit):
	  <ul class="action-items">
	    <li><a target="_blank" href="https://mcgill.on.worldcat.org/oclc/1348479214">Munn et al., 2022</a></li>
	    <li><a target="_blank" href="https://mcgill.on.worldcat.org/oclc/1350079336">Mehta et al/. 2023</a></li>
	    <li><a target="_blank" href="https://mcgill.on.worldcat.org/oclc/1289339048">Kamath and Lui, 2021</a></li>
	  </ul>
	</p>
	<h2 class="topic-heading">Concepts</h2>
	<ul class="flex-horizontal border-none">
	  <li>explainability</li>
	  <li>trust</li>
	  <li>XAI</li>
	</ul>	
    </section>
    <footer>
      <p>
	Copyright Â©
	<script>
	  document.write(new Date().getFullYear());
	</script>
	McGill University
      </p>
    </footer>
    <script src="../../jquery-3.3.1.slim.min.js"></script>
    <script src="../../popper.min.js"></script>
    <script src="../../bootstrap.min.js"></script>
    <script src="../../scripts.min.js "></script>
  </body>
</html>
