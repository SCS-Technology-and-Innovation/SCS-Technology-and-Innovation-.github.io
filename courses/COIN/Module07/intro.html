<!DOCTYPE html>
<HTML lang="en">
  <HEAD>
    <META HTTP-EQUIV="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="../../all.min.css">
    <link rel="stylesheet" href="../../styles.min.css">
    <link rel="stylesheet" href="../../custom.css">
    <link rel="stylesheet" href="../../style-2022.css">
    <title>Introduction / Module 7 / CCCS 660</TITLE>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-F8SJ1WFY9N"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){
	  dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "G-F8SJ1WFY9N");
    </script>
  </head>
  <body class="content" role="document">
    <section class="bg-">
      <div class="container-fluid">      
	<h1>Introduction</h1>
	<p>
	  Public discourse on AI systems tends to be centered around
	  machine-learning approaches. One of the dominant techniques
	  for machine learning is that of neural networks in which
	  layers of perceptrons (see the previous module) are stacked
	  one after another to solve problems that cannot be
	  satisfactorily solved with a simple combination of linear
	  separations of the inputs.
	</p>
	<p>
	  We could of course build such a system from simple
	  perceptrons, feeding the outputs of the first layer into the
	  second one as inputs and so on, but the modern-day approach
	  is less artisanal: deep learning refers to combining layers
	  with different, predefined internal workings (a bit like
	  LEGO bricks) to build a pipeline of sorts.
	</p>
	<h2 class="topic-heading">Learning outcomes</h2>
	<p>
	  This module will help you do the following:
	  <ul class="action-items">
	    <li>Train a perceptron</li>
	    <li>Combine several perceptrons to encode a multi-class problem</li>
	    <li>Vary the input dimension of a perceptron </li>
	    <li>Encode inputs to improve linear separability</li>
	  </ul>
	</p>
	<h2 class="topic-heading">Warm-up</h2>
	<p>
	  Spending a few minutes before live class with the Baheti's 
	  <a target="_blank"
	     href="https://www.v7labs.com/blog/neural-network-architectures-guide">The
	    Essential Guide to Neural Network Architectures</a> will help
	  make sense of what we discuss.
	</p>
	<h2 class="topic-heading">Concepts</h2>
	<p>
	  After this module, you should be familiar with the following concepts:
	  <ul class="action-items">
	    <li>Neural network (NN)</li>
	    <li>Backpropagation</li>
	    <li>NN architecture</li>
	    <li>Deep learning</li>
	    <li>True/false positive/negative</li>      
	    <li>Precision, recall, accuracy, F1 score</li>
	  </ul>
	</p>	
	<p>
	  Remember that you can always look concepts up in the
	  <a target="_blank"
	     href="https://scs-technology-and-innovation.github.io/courses/TI.html">glossary</a>.
	  Should anything be missing or insufficient,
	  please <a target="_blank"
		    href="https://forms.office.com/r/VZpfFrZhRu">report</a> it.
	</p>
    </section>
    <footer>
      <p>
	Copyright Â©
	<script>
	  document.write(new Date().getFullYear());
	</script>
	McGill University
      </p>
    </footer>
    <script src="../../jquery-3.3.1.slim.min.js"></script>
    <script src="../../popper.min.js"></script>
    <script src="../../bootstrap.min.js"></script>
    <script src="../../scripts.min.js "></script>
  </body>
</html>
